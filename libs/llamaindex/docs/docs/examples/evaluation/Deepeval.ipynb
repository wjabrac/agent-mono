{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/Deepeval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ RAG/LLM Evaluators - DeepEval\n",
    "\n",
    "This code tutorial shows how you can easily integrate DeepEval with LlamaIndex. DeepEval makes it easy to unit-test your RAG/LLMs.\n",
    "\n",
    "You can read more about the DeepEval framework here: https://docs.confident-ai.com/docs/getting-started\n",
    "\n",
    "Feel free to check out our repository here on GitHub: https://github.com/confident-ai/deepeval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up and Installation\n",
    "\n",
    "We recommend setting up and installing via pip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -q llama-index\n",
    "!pip install -U -q deepeval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is optional and only if you want a server-hosted dashboard! (Psst I think you should!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepeval login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Metrics\n",
    "\n",
    "DeepEval presents an opinionated framework for unit testing RAG applications. It breaks down evaluations into test cases, and offers a range of evaluation metrics that you can freely evaluate for each test case, including:\n",
    "\n",
    "- G-Eval\n",
    "- Summarization\n",
    "- Answer Relevancy\n",
    "- Faithfulness\n",
    "- Contextual Recall\n",
    "- Contextual Precision\n",
    "- Contextual Relevancy\n",
    "- RAGAS\n",
    "- Hallucination\n",
    "- Bias\n",
    "- Toxicity\n",
    "\n",
    "[DeepEval](https://github.com/confident-ai/deepeval) incorporates the latest research into its evaluation metrics, which are then used to power LlamaIndex's evaluators. You can learn more about the full list of metrics and how they are calculated [here.](https://docs.confident-ai.com/docs/metrics-introduction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Setting Up Your LlamaIndex Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Read LlamaIndex's quickstart on more details, you will need to store your data in \"YOUR_DATA_DIRECTORY\" beforehand\n",
    "documents = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "rag_application = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Using DeepEval's RAG/LLM evaluators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepEval offers 6 evaluators out of the box, some for RAG, some directly for LLM outputs (although also works for RAG). Let's try the faithfulness evaluator (which is for evaluating hallucination in RAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.integrations.llamaindex import DeepEvalFaithfulnessEvaluator\n",
    "\n",
    "# An example input to your RAG application\n",
    "user_input = \"What is LlamaIndex?\"\n",
    "\n",
    "# LlamaIndex returns a response object that contains\n",
    "# both the output string and retrieved nodes\n",
    "response_object = rag_application.query(user_input)\n",
    "\n",
    "evaluator = DeepEvalFaithfulnessEvaluator()\n",
    "evaluation_result = evaluator.evaluate_response(\n",
    "    query=user_input, response=response_object\n",
    ")\n",
    "print(evaluation_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full List of Evaluators\n",
    "\n",
    "Here is how you can import all 6 evaluators from `deepeval`:\n",
    "\n",
    "```python\n",
    "from deepeval.integrations.llama_index import (\n",
    "    DeepEvalAnswerRelevancyEvaluator,\n",
    "    DeepEvalFaithfulnessEvaluator,\n",
    "    DeepEvalContextualRelevancyEvaluator,\n",
    "    DeepEvalSummarizationEvaluator,\n",
    "    DeepEvalBiasEvaluator,\n",
    "    DeepEvalToxicityEvaluator,\n",
    ")\n",
    "```\n",
    "\n",
    "For all evaluator definitions and to understand how it integrates with DeepEval's testing suite, [click here.](https://docs.confident-ai.com/docs/integrations-llamaindex)\n",
    "\n",
    "## Useful Links\n",
    "\n",
    "- [DeepEval Quickstart](https://docs.confident-ai.com/docs/getting-started)\n",
    "- [Everything you need to know about LLM evaluation metrics](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
